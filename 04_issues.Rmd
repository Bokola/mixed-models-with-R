# Issues


## Variance accounted for

People really love the R-squared value that comes from standard regression.  Never mind that it is inherently biased, nor does it matter that there is no way to state what would be a 'good' result for a given data situation, nor that many of these same people actually don't know how to interpret it, nor does it even matter that the same kinds of folks have no qualms about dropping it from a report at the first sign of trouble.

Suffice it to say that when there are multiple sources of 'variance', talking about *variance accounted for* is not straightforward.  Still, many have tried. You might look at [this package](https://cran.r-project.org/web/packages/r2glmm/) and the references noted in the description.  See also the [GLMM FAQ](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#how-do-i-compute-a-coefficient-of-determination-r2-or-an-analogue-for-glmms).  I would suggest not even bothering beyond the standard linear mixed model.

## Alternative approaches to mixed models

I have a [document](https://m-clark.github.io/docs/clustered/) that goes into more detail about many approaches to dealing with clustered data, but we can briefly talk about some here. Common alternatives used in clustered data situations include:

- Fixed effects models (also panel linear models with fixed, as opposed to random, effects)
- Using cluster-robust standard errors
- Generalized estimating equations (GEE)

The first two are commonly used by those trained with an econometrics perspective, while you might see GEE more with those of a biostatistics or other perspective. GEE are in fact a generalization of the cluster-robust approach, and extend generalized least squares (GLS) to nonlinear/GLM settings.  The nature of fixed effects models allow you to control for, but not investigate, any cluster level effects.  This makes them a non-starter for many investigations, as those are typically of prime theoretical interest.  GEE approaches allow one to take into account the dependency in the data, but ignore what might be very interesting, i.e. the random effects and associated variance. There are also few tools for GEE in more complicated correlational structures beyond a single clustering variable.


### Growth curve models

With longitudinal data, growth curve models are a latent variable approach that is commonly used in these situations. With appropriate setup, they will duplicate the results of a mixed model.  In my opinion, there are few reasons to use a growth curve approach over a mixed model, and many reasons not to, not least of which is that effects which would be simple to interpret in the mixed model approach are now a source of confusion to applied researchers in the growth curve model, even though it's the same thing.  Furthermore, indirect effects, growth mixture models and other extensions common in the latent variable approach are more easily implemented in the mixed model approach.  In short, only the most complicated models would perhaps require a growth curve model, but would also bring with it many other complications.  See more [here](https://m-clark.github.io/docs/sem/latent-growth-curves.html).



## Sample sizes

### Small number of clusters

Think about how many values of some variable you'd need before you felt comfortable with statistics based on it, especially standard deviation/variance.  That's at play with mixed models, in the sense you'd like to have enough groups to adequately assess the variance components. Mixed models will run with very small numbers, though the estimates will generally be biased.  I have a demo [here](https://m-clark.github.io/docs/mixedModels/growth_vs_mixed_sim.html) if interested.  One way to deal with this is to move to the Bayesian context, which will automatically induce some regularization  in parameter estimates.

This also speaks to the issue some will have regarding whether they should treat something as a <span class="emph">fixed vs. random</span> reffect.  Historical definitions would unnecessarily restrict usage of random effects approaches.  For example, random effects were defined to be a (random) sample from some population.  If this were the case some might take issue when your levels do not deal with a sample, but the whole population, as in the case where your cluster is state and you have all 50 states.  This doesn't matter.  If you have enough levels to consider a mixed model approach, feel free to do so.


### Small N within cluster

Mixed models work even with no more than two in each cluster and some singletons. Even in the simple case of pre-post design, mixed models are entirely applicable, though limited (you can't have random slopes).  So whenever you have clustering of some kind, you should consider mixed models.


### Balanced/Missing values

We've primarily been looking at <span class="emph">balanced</span> data, where each clusters have the same number of observations within them.  There is no requirement for this, and in many cases we wouldn't even expect it, e.g. people within geographical units.

However, if data is only missing on the outcome, or a mix of variables, we essentially have the same issue as with typical data situations, and will have the same considerations for dealing with missingness.  If you don't lose much data, the practical gain by ignoring missingness generally outweighs the complexities that can come with, for example, multiple imputation[^mi], even in the best of settings. By default, mixed models assume missing at random.  On the other hand, longitudinal data has special considerations, as there is typically increasing dropout over time.

Having dealt with missingness in a variety of contexts with different approaches (FIML, MI, Bayesian), the end result is usually that you spend vast amounts more time dealing with the missing data than you do understanding your models, and yet don't feel any better about the results.  Unless the missingness would make you lose a good chunk of the data, it's probably best just to leave that to the limitations section of your report[^missingreviewer].  If you do deal with it, I'd suggest an approach that is essentially a one-off (e.g. with <span class="pack">missForest</span>) to be compared to the data that ignores the missingness, but still allows you to do everything you want.  While you may not incorporate all sources of uncertainty in doing so, it seems to me a viable compromise.


## Model Comparison

Model comparison takes place in the usual way in the sense of potentially having statistical tests and information criteria.  Unfortunately, the typical likelihood ratio tests one might use in standard settings are not so straightforward here. For example, at a minimum you'd have to change the default estimation from REML to ML, and the models must have the same random effects structure, in order to compare models with different fixed effects for the resulting test p-value to be correct.  It works the other way to compare models with different random effects structure.

In my opinion, model selection involves considerations of theory, parsimony, and prediction, and those tests do not. I'm not partial to such tests even in the standard setting, and would use AIC here to potentially aid (not make) a model choice if I thought it was necessary, as I would there[^lrtest]. In general though, trying to determine a 'best' model with one set of data is a problematic endeavor at best, and at worst, completely misguided.  I think it's very useful to build models of increasing complexity, and select one to focus on based on the available evidence.  Just don't get hung up on choosing one based solely on the outcome of a single statistic.  If you have a lot of data, you should consider some sort of explicit validation approach if you really want to compare competing models, but that is not without complication given the dependency in the data.



[^mi]: Multiple imputation is straightforward only in theory.  In practice it becomes a major pain to go very far beyond getting the parameter estimates for simple models.  Full information maximum likelihood (FIML) is little implemented outside of SEM software/packages, and more problematic in its assumptions.

[^lrtest]: If you really want them see the <span class="pack">lmertest</span> package.  Note also that AIC [does not come with a free lunch](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#can-i-use-aic-for-mixed-models-how-do-i-count-the-number-of-degrees-of-freedom-for-a-random-effect).  See the <span class="pack">cAIC4</span> package and references therein.

[^missingreviewer]:  Just note that in some disciplines, the reviewers, who will never actually do this themselves for the same reasons, nevertheless will make a big deal about the missing data because it's an easy point for them to make.  This is similar to economics reviewers who shout 'endogeneity!' at every turn, but won't bother to tell you where to get the instrument in the first place, admit that IV analysis is problematic in its own right, or what the approach should be in complex model settings such as mixed models with nonlinear, spatial effects, multiple levels of clustering etc.