# Clustered Data



## Random Intercepts demo
d and crossed (respectively) to deal with these two scenarios.


## Example: student popularity

For the following we'll assess student popularity.  Each student rates the others, and so observed values are clustered within raters.  The popularity metric is 1-10, and we will include sex of the rater as a covariate.  See the [appendix][Appendix] for details.  

```{r popular_setup, echo=FALSE, eval=FALSE}
popularity = haven::read_dta('data/joop_hox_data/ma_hox_stata/popular.dta')
popularity = popularity %>% 
  mutate(sex = factor(sex, labels = c('Male', 'Female'))) %>% 
  select(-const)
save(popularity, file='data/popularity.RData')
# glimpse(popularity)
# summary(popularity)
```

The model can be depicted in different ways.

$$popularity = b_{\mathrm{intercept}} + b_{\mathrm{sex}}\cdot sex + (\mathrm{effect}_{\mathscr{rater}} + \epsilon)$$
$$popularity = b_{\mathrm{rater}} + b_{\mathrm{sex}}\cdot sex + \epsilon$$
$$b_{\mathrm{rater}} = b_{\mathrm{intercept}} + \mathrm{effect}_{\mathrm{rater}}$$
In either case, we assume the following for the rater effects and error term.

$\epsilon \sim \mathscr{N}(0, \sigma)$
$\mathrm{effect}_{\mathrm{rater}} \sim \mathscr{N}(0, \tau)$

In other words, conceptually the only difference between this and a standard regression is the rater effect, which is on average no effect, but specifically varies from rater to rater with some standard devation ($\tau$).

Such a model is easily conducted in R, specifically with the package <span class="pack">lme4</span>.  In the following, the code will look just like what you used for regression with lm, but with an additional component specifying the group effect.  The `(1|pupil)` means that we are allowing the intercept, represented by 1, to vary by pupil. As a starting point, we'll also do the basic regression.

```{r popular_lme4}
load('data/popularity.RData')
library(lme4)
popular_lm = lm(popular ~ sex, data=popularity)
popular_mixed = lmer(popular ~ sex + (1|pupil), data=popularity)
```


First, we'll look at the regression.  We see that Females have a higher score on average.  When we look at the residuals, where we can see the student grouping effect.

```{r reg_output}
summary(popular_lm)
qqnorm(residuals(popular_lm))
```

With the mixed model, we get the same results as the regression, but with more to talk about.

```{r mixed_output}
summary(popular_mixed)
```

In the above


